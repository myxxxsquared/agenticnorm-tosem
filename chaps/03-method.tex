\section{Method}

\input{floats/03-01-method-overview.tex}

In this section, we present \lighttechname, a lightweight multi-agent anomaly detection framework for web applications.
\lighttechname is designed to overcome the limitations of prior solutions such as WebNorm, namely the reliance on heavyweight closed-source models, sensitivity to prompt engineering, and difficulty in handling long log contexts.
Figure~\ref{fig:method_overview} provides an overview of the workflow.

\subsection{Workflow}

A central challenge in log-based anomaly detection is that prompt quality strongly influences detection results. Fixed prompts are brittle and may fail to capture certain invariants, leading to missed anomalies. To address this limitation, we propose an iterative loop in which attack generation and prompt adjustment are tightly coupled. The loop continuously strengthens prompts by exposing them to adversarial scenarios that exploit their current weaknesses. This process consists of three main modules, forming an iterative loop (Figure~\ref{fig:method_overview}):

\lighttechname consists of three main modules:
\begin{itemize}
    \item \textbf{Constraint Learning}: derives invariants from normal logs.
    \item \textbf{Attack Generation}: synthesizes abnormal logs that break or bypass the learned invariants.
    \item \textbf{Prompt Refinement}: updates LLM prompts using feedback from undetected attacks.
\end{itemize}

\lighttechname begins by deriving invariants from normal logs using an initial prompt in the \textbf{Constraint Learning} module.
Then, the \textbf{Attack Generation} module synthesizes abnormal logs that break or bypass the learned invariants.
Finally, the \textbf{Prompt Refinement} module updates LLM prompts using feedback from undetected attacks.

This adversarial loop allows prompts to evolve dynamically. Each cycle expands the attack space by introducing logs that specifically target the weaknesses of the current invariants, and in turn strengthens the prompts by incorporating counterexamples. Over time, this reduces reliance on manual intervention and improves robustness against both known and novel attacks.

Next, we break down each module in detail.

\subsection{Constraint Learning}

\input{floats/03-02-method-constraintlearning.tex}

\lighttechname generally follows the idea of WebNorm, but differs in that it does not rely on source code or data-flow analysis.
This requires us to replace several of its original components.
Figure~\ref{fig:method_constraint} illustrates the process of constraint learning.
First, \lighttechname discovers relationships between APIs through frequency-based analysis.
Next, to adapt to lightweight LLMs, \lighttechname applies \emph{Field Clustering}, which reduces the length of the input context per query, thereby lowering the workload of the model while improving its ability to identify constraints.
Finally, \lighttechname adopts a similar approach to WebNorm for detecting both intra-API and inter-API constraints, using an LLM to extract invariants and generate corresponding Python checking code.
Unlike WebNorm, however, the prompts employed here are not manually designed; instead, they are obtained from the iterative refinement process described later, making them better suited for lightweight LLMs.

\subsubsection{Frequency Analysis}

\lighttechname employs a frequency-based method to identify related APIs, eliminating the need for program analysis.
Specifically, for a given API, it scans the surrounding window of log entries and counts the frequency of co-occurring API calls.
The top-$K$ most frequent co-occurrences are considered related APIs, thus establishing inter-API relations.
After this step, we utilize an LLM to verify and filter out spurious relations.

\input{floats/03-04-freq}

Figure~\ref{fig:method-freq} shows an example of frequency-based analysis on the \trainticket dataset.
Given a list of API calls, we slide a window of size $K$ and count the frequency of co-occurring APIs.
For instance, in this case, \texttt{CreateOrder} and \texttt{AddPassenger} frequently appear together, indicating a potential relationship.
Then, the LLM is used to verify and filter out spurious relations.

\subsubsection{Field Clustering}

Lightweight LLMs are constrained by limited context windows, making it infeasible to directly process lengthy and complex logs. To address this limitation, we introduce \emph{field clustering}, a technique that decomposes log entries into semantically related groups. This allows invariants to be extracted while ensuring that the input remains within the restricted context length.

To this end, \lighttechname first analyzes the structure of logs, which often contain nested dictionaries and arrays. It then applies a set of expansion rules to flatten these structures into atomic fields. Finally, it employs an LLM to cluster the expanded fields based on semantic relatedness, forming compact groups that can be processed within the context limits.

Figure~\ref{fig:motivating_example_clustering} provides an example of the field clustering process. The original log contains nested dicts (e.g., \texttt{arguments}, \texttt{qi}, etc.) and arrays (e.g., \texttt{results}). These structures are first expanded into flat fields (e.g., \texttt{arguments.loginId}, \texttt{arguments.orderId}, etc.). Finally, the expanded fields are clustered into semantically related groups (e.g., the cluster of \texttt{arguments.loginId}, \texttt{env.userId}, \texttt{qi.accountId} represents user identifiers).

Formally, we show the field clustering process in three steps: Log Structure Discovery, Expansion, and Clustering.
Log Structure Discovery identifies the schema of logs and their data types.
Expansion applies a set of rules to flatten nested structures into atomic fields.
Clustering groups the expanded fields into semantically related clusters using an LLM.
