\section{Experiments}

We focus on the following research questions.

\begin{itemize}
	\item \textbf{RQ1: Overall Performance.}
	      How effective is \lighttechname in detecting web tamper attacks compared to state-of-the-art baselines and WebNorm?
	      We evaluate its precision, recall, and F1-score on standard benchmarks.

	\item \textbf{RQ2: Ablation Study.}
	      How do the core components of \lighttechname contribute to its performance?
	      We conduct ablation experiments on field clustering, attack generation, and prompt refinement to measure their individual impact.

	\item \textbf{RQ3: Model Scalability.}
	      How does \lighttechname perform when deployed with different scales of lightweight, locally deployable LLMs?
	      We assess the trade-offs between detection accuracy, efficiency, and resource consumption across small, medium, and larger models.

	\item \textbf{RQ4: Direct Substitution.}
	      What happens if WebNorm is directly replaced with a smaller LLM without architectural modifications?
	      This comparison highlights the necessity of our proposed techniques over naïve model substitution.
\end{itemize}

\subsection{Experimental Setup}

\paragraph{Benchmarks.}
We evaluate our approach on two widely-used benchmarks of web application logs:
\trainticket and \nicefish.
Both datasets contain normal and attack traces derived from real-world systems,
with injected tampering behaviors that allow controlled evaluation.
Following prior work, we split logs into fixed-size windows of 20 entries,
and assign binary labels at the window level.

\paragraph{Baselines.}
To demonstrate the effectiveness of \lighttechname,
we compare against three categories of methods:
(1) \textit{learning-based baselines}, including LogRobust~\cite{zhang2019robust},
LogFormer~\cite{guo2024logformer}, and RAPID FastLogAD~\cite{lin2024fastlogad},
which rely on supervised or semi-supervised learning of log sequences;
(2) \textit{rule-based approaches}, represented by WebNorm~\cite{liao2024detecting},
the current state-of-the-art interpretable system for normality modeling.
These baselines cover both predictive and rule-driven paradigms in log anomaly detection.

\paragraph{Evaluation Metrics.}
We adopt precision and recall as the primary metrics.
For windows of normal logs, if any attack is incorrectly flagged, the window is counted as a false positive (FP); otherwise it is a true negative (TN).
For attack-containing windows, the detection of any injected attack is considered a true positive (TP), otherwise it is a false negative (FN).
Formally, precision, recall, and F1-score are computed as
$$\text{Precision} = \dfrac{TP}{TP + FP}, \quad
\text{Recall} = \dfrac{TP}{TP + FN}, \quad
\text{F1} = \dfrac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.$$

\subsection{Results and Analysis}

\input{floats/04-01-overall}

\paragraph{RQ1: Overall Performance.}
Table~\ref{tab:overall-eval} summarizes the overall comparison\footnote{We note that our reproduced results of WebNorm differ slightly from those reported in the original paper. After contacting the authors, we confirmed that they updated their dataset, which improved precision but reduced recall. The results shown here reflect this corrected version.}.
\lighttechname achieves the highest F1-scores on both \trainticket (0.92) and \nicefish (0.95).
Some baselines, such as LogFormer and RAPID, obtain relatively high recall
(e.g., 0.90 on \trainticket and 1.00 on \nicefish for RAPID),
but this comes at the cost of extremely low precision (0.11 / 0.04),
leading to many false alarms.
LogFormer offers a more balanced trade-off, but its F1-scores (0.40 / 0.42) remain far lower than ours.
By contrast, WebNorm achieves perfect precision (1.00) but suffers from lower recall
(0.80 on \trainticket and 0.75 on \nicefish),
missing many real attacks due to its reliance on fixed rules.
\lighttechname preserves the perfect precision of WebNorm while substantially improving recall
(0.86 / 0.92), thus delivering the strongest overall detection performance.

\finding{\textbf{RQ1:} \lighttechname surpasses state-of-the-art baselines, achieving the best F1-scores across both benchmarks.}

\input{floats/04-02-ablation}

\paragraph{RQ2: Ablation Study.}
Table~\ref{tab:ablation-study} reports the impact of removing each component.
All three modules contribute to performance improvements, but their effects differ in magnitude.
Field clustering proves most critical: removing it reduces the F1-score from 0.92 to 0.60 on \trainticket
and from 0.95 to 0.83 on \nicefish.
Prompt refinement has a comparable impact, with F1 dropping to 0.61 and 0.83, respectively.
By contrast, removing API relation prediction leads to smaller but still notable degradation
(0.92 $\rightarrow$ 0.67 on \trainticket and 0.95 $\rightarrow$ 0.50 on \nicefish),
showing that it provides complementary benefits.

\input{floats/04-04-tokens}

To further evaluate the effectiveness of field clustering, we analyze the total number of input tokens in the prompts, comparing settings with and without clustering.
Table~\ref{tab:tokens-comparison} reports the token counts for each invariant generation task. The reduction is particularly pronounced on \trainticket, as its logs contain more fields, and clustering eliminates a larger portion of redundancy. By shortening the token length, the model can process inputs more efficiently, which in turn leads to higher-quality invariants.

\finding{\textbf{RQ2:} Each component of \lighttechname improves performance, with field clustering and prompt refinement being especially crucial.}

\input{floats/04-03-models}

\paragraph{RQ3: Comparison between Different LLMs.}
Table~\ref{tab:comparing-llms} shows results when varying the LLM used for the
\emph{Constraint Learning} module, while keeping \emph{Attack Generation} and
\emph{Prompt Refinement} fixed to DeepSeek-V3.
Across all four models (\texttt{DeepSeek-V3}, \texttt{Gemma 3 4B}, \texttt{GPT-OSS 20B}, \texttt{GPT-OSS 120B}),
precision remains consistently perfect (1.00),
and recall varies only slightly
(0.83--0.86 on \trainticket and 0.90--0.95 on \nicefish).
This indicates that the effectiveness of \lighttechname is not tied to a specific
model scale in the constraint learning stage.
The invariants derived through clustering and refinement are robust across models,
demonstrating that smaller and more efficient LLMs can be deployed in practice
without sacrificing detection accuracy.

\finding{\textbf{RQ3:} \lighttechname maintains high performance across different LLMs, confirming its adaptability to smaller, locally deployable models.}

\paragraph{RQ4: Direct Substitution.}
To further validate our design, we directly substitute WebNorm’s backbone with a smaller LLM (e.g., \texttt{DeepSeek V3}),
without applying any of our proposed architectural modifications.
Performance drops sharply in recall: on \trainticket, recall falls from 0.92 to 0.60, and on \nicefish, from 0.95 to 0.50.
This experiment shows that naïvely replacing large models with smaller ones is insufficient.
While WebNorm functions well with powerful external LLMs, its invariants are too brittle when scaled down.
By contrast, our techniques—field clustering, attack generation, and prompt refinement—enable small models to remain competitive, supporting practical local deployment.

\finding{\textbf{RQ4:} Simply substituting smaller LLMs into WebNorm leads to severe performance degradation, highlighting the necessity of our architectural innovations for making lightweight deployment viable.}
