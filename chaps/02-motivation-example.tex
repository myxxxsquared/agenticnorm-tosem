\section{Motivating Example}

\input{floats/02-01-motivation-example}


To illustrate the challenges of detecting anomalies from API logs, we consider an anomaly case extracted from the \trainticket dataset.
We show an attack case from \trainticket dataset and focus on two APIs that appear in its backend logs:
\texttt{/api/v1/queryOrders} (\texttt{queryOrders}) and \texttt{/api/v1/cancelOrder} (\texttt{cancelOrder}).
Figure~\ref{fig:attack_refinement} shows a simplified version of the relevant log entries. The full version of these logs are avialable in our anonymous artifact~\cite{agenticnorm-website}. The original logs are very long and in JSON format.

In a normal ticket-cancellation workflow, the operation consists of two steps.
First, the user invokes \texttt{/api/v1/queryOrders} (abbreviated as \texttt{queryOrders})
to retrieve the list of refundable tickets.
Second, the user selects one of the returned tickets and issues a cancellation request
through \texttt{/api/v1/cancelOrder} (abbreviated as \texttt{cancelOrder}).
In this normal case, the \texttt{orderId} chosen for cancellation (e.g., \texttt{fe9c72d9})
must come from the list returned by \texttt{queryOrders}.

However, because the frontend code is executed entirely on the client side,
a malicious user can tamper with the browser data and craft an unauthorized request.
For example, the attacker may replace the legitimate \texttt{orderId}
(\texttt{fe9c72d9}) with an arbitrary identifier not present in the queried list
(e.g., \texttt{418ea03c}).
This allows the user to cancel a ticket they do not own, or to repeat a cancellation
that should not be permitted. Such behavior can result in duplicate refunds and
financial losses for the system, creating serious security and integrity risks.

\paragraph{Existing Approaches}
There are two main categories of existing approaches for detecting anomalys in web logs:
(1) \textbf{Model-learning-based detectors} that learn embeddings and features from both normal and abnormal logs and classify anomalies based on learned patterns; and
(2) \textbf{Invariant-learning-based detectors} that learn semantic invariants from logs and flag violations as anomalies.

Model-learning-based detectors that rely on embeddings and features struggle here: the abnormal and normal logs are extremely similar, differ in only a few fields, and are embedded very close in vector space.
Moreover, the end-to-end log sequence is long and interleaved with irrelevant events, so the anomaly signal is diluted by normal traffic.


To address this, prior work WebNorm~\cite{webnorm} proposes to learn semantic invariants from web logs using LLMs.
WebNorm can catch this attack by learning a constraint between two APIs:
\texttt{cancelOrder.arguments.orderId} must match one of \texttt{queryOrders.results[].id}.
Very briefly, it maps code‐level data flows to log fields and asks an LLM to confirm such relations as invariants,
then verifies at runtime whether the collected logs satisfy them; any mismatch is flagged as an anomaly.
For this motivating example, WebNorm learns the cross-API data constraint. Violation of this invariant flags the attack.

Despite its effectiveness, WebNorm has three limitations:
\begin{itemize}
    \item \textbf{Program-analysis and source-code dependence}: it requires access to and instrumentation of frontend/backed code to build log-code mappings, which is costly, brittle under rapid changes, and infeasible for closed-source or third-party components.
    \item \textbf{Heavyweight LLMs}: constraint confirmation/synthesis relies on closed source property models, adding latency, cost, and compliance concerns, and suffering from privacy issues when sending data to remote servers. The original logs are very long, often exceeding the context window of compact models, making it impossible to process them in full.
    \item \textbf{Prompt sensitivity}: the right constraint often emerges only when the prompt explicitly elicits it, leading to manual, project-specific prompt engineering with limited transferability and unstable results.
\end{itemize}

\paragraph{Our Approach}

We aim to retain WebNorm’s \emph{consistency-constraint} power while eliminating its limitations.
Our approach uses only logs (no source or program analysis), runs primarily with compact, locally deployable LLMs, and mitigates prompt fragility by automatically refining prompts with generated attacks.

To this end, we propose two techniques for anomaly detection with compact models:  
(1) \emph{Field Clustering}: a log entity is decomposed into multiple small clusters, each containing only related and comparable fields. The large model processes one cluster at a time, which greatly alleviates the context length limitation and makes it easier to capture relationships among fields;  
(2) \emph{Prompt Refinement via Generated Attacks}: prompts are designed to guide the LLM to automatically generate abnormal logs. These abnormal logs are then used to check whether the current prompt misses certain invariants. If omissions are found, the LLM refines the prompt, and the refined prompt is subsequently used by the compact model to regenerate invariants.

\paragraph{Field Clustering}
To fit compact models and make comparisons explicit, we expand the original JSON log entity into flatten fields, finding potential relationships and grouping comparable fields into small, semantically coherent clusters rather than feeding the full log. Figure~\ref{fig:motivating_example_clustering} illustrates this process. For each cluster, we create a separate input for the LLM to process. The clusters are designed to capture fields that should be compared or checked for consistency.

\input{floats/02-02-motivation-example-clustering}

\paragraph{Prompt Refinement via Generated Attacks}

\input{floats/02-03-motivation-example-prompt}

Small locally deployable models generally have limited reasoning ability, and thus require explicitly specified relationships between fields to capture their constraints. However, manually adjusted prompts are difficult to generalize across diverse scenarios. Figure~\ref{fig:motivating_example_prompt} (left) shows the original WebNorm prompt, which relies on manually crafted instructions and examples, making prompt adjustment highly labor-intensive. Such prompts struggle to incorporate project-specific constraints or cover a broader range of scenarios. When invariants are generated directly by compact models, their weak reasoning ability prevents them from capturing field-level constraints using these prompts alone. Figure~\ref{fig:motivating_example_prompt} (middle) illustrates our modified prompt, where we add new instructions that guide the LLM to generate abnormal logs. These added instructions encourage the model to reason about constraints between fields—for example, the second instruction requires checking the relationships between joined fields and their original fields, which proves effective in our motivating example by correctly identifying the necessary field matching. Figure~\ref{fig:motivating_example_prompt} (right) shows the invariant generated from the refined prompt, which successfully captures the intended field-level constraints.

