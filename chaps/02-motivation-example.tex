\section{Motivating Example}

To illustrate the challenges of inferring invariants from API logs, we consider an anomaly case extracted from the \emph{TrainTicket} dataset. 

\paragraph{Example Scenario.} 
Suppose a user issues two consecutive API calls: \texttt{queryOrders} to retrieve their own order history, followed by \texttt{cancelOrder} to cancel a ticket. The corresponding log entries are shown below.

A natural invariant between these two APIs is that \texttt{cancelOrder.arguments.orderId} must match one of the order identifiers returned by \texttt{queryOrders.response.data.id}. This invariant enforces that users can only cancel tickets they actually own. 

In the anomaly observed in the TrainTicket dataset, however, this constraint is violated: a user is able to cancel another person’s ticket by providing an arbitrary order identifier. If deployed in a real system, such behavior could enable malicious users to obtain illegitimate refunds or disrupt other passengers’ orders. WebNorm can in principle detect this anomaly, but only if the prompt explicitly specifies the relationship that \texttt{cancelOrder.arguments.orderId} should match one of \texttt{queryOrders.response.data.id}. This highlights the difficulty of reliably capturing such invariants without manual intervention. 

\paragraph{Challenges for Small Models.} 
Automatically discovering semantic invariants from web logs is non-trivial. Small models in particular face two fundamental challenges:

\begin{itemize}
    \item \textbf{Challenge 1: Limited Context Capacity.}  
    Real-world web logs are often very long due to nested JSON structures and verbose API responses. Even the motivating example can easily span hundreds of tokens, while a single \texttt{queryOrders} response may contain dozens of orders, each with multiple fields. As a result, many logs exceed the maximum context window of compact LLMs, making it impossible to process them in full. Table~\ref{tab:log_length} quantifies this issue by showing the proportion of logs that surpass both the global maximum context length and the local capacity of a high-end compact model (Gemma3-27B). Hardware memory further tightens this constraint: for instance, a 27B-parameter model running on a 49GB GPU typically supports only 16k–32k tokens, which is still insufficient for a non-negligible fraction of logs.

    \item \textbf{Challenge 2: Reliance on Prompt Engineering and Model Reasoning.}  
    Existing approaches such as WebNorm depend heavily on carefully crafted prompts and the reasoning power of large models. In practice, small models exhibit much weaker log comprehension: they often fail to capture cross-API relationships unless the prompt explicitly spells out the attack scenario. For example, detecting that \texttt{cancelOrder.arguments.orderId} must match one of \texttt{queryOrders.response.data.id} requires stating this condition directly in the prompt; otherwise, the model cannot infer the invariant. Simply replacing the large model with a smaller one thus leads to significant drops in detection accuracy.
\end{itemize}


% \paragraph{Hardware Capacity Analysis.}
% When deploying locally, GPU memory further constrains the usable context length. Consider a single NVIDIA RTX 6000 Ada GPU with \textbf{49140MiB} memory. Running a 27B-parameter model such as \textbf{Gemma3-27B}, the maximum supported context window is roughly
% \[
% \text{MaxTokens} \approx \frac{49{,}140 \text{ MB}}{\text{Memory per token}}.
% \]
% Following standard scaling estimates, Gemma3-27B on 49GB VRAM can typically accommodate around \textbf{16k--32k tokens} in full precision, depending on batch size and precision optimizations. However, our empirical distribution shows that a non-negligible fraction of logs already exceed this limit. The last column of Table~\ref{tab:log_length} quantifies this effect. 

\paragraph{Our Solution.}
To address these challenges, we propose two complementary strategies:
\begin{enumerate}
    \item \textbf{Field Clustering.} To mitigate context length issues, we cluster comparable fields together. Instead of providing the full log, we expand the structures and extract only relevant fields for invariant inference.
    \item \textbf{Refined Prompts via Generated Attacks.} To strengthen reasoning, we refine prompts by leveraging automatically generated attacks. These adversarial cases expose missing invariants, which in turn help the model generalize and infer more comprehensive rules.
\end{enumerate}

This motivating example highlights both the importance of semantic invariants in API interactions and the inherent difficulties faced by small models, motivating the need for our proposed approach.

\paragraph{Field Clustering with \lighttechname.}
To overcome the context length limitation, our approach, \lighttechname, introduces a \emph{field clustering} mechanism. 
Instead of feeding the entire long log into the model, we decompose the original log into a set of smaller clusters. 
Each cluster groups together only the fields that are semantically comparable across APIs. 

For example, in the motivating case of \texttt{cancelOrder} and \texttt{queryOrders}, the raw log is long and contains numerous nested fields. 
\lighttechname\ automatically partitions these fields into multiple clusters:
\begin{itemize}
    \item \textbf{Cluster 1:} focuses on order identifiers, linking \texttt{cancelOrder.arguments.orderId} with \texttt{queryOrders.response.data.id}.
    \item \textbf{Cluster 2:} focuses on status values, linking \texttt{cancelOrder.response.status} with \texttt{queryOrders.response.data.status}.
    \item \textbf{Cluster 3:} focuses on user and account identity, linking \texttt{cancelOrder.arguments.loginId}, \texttt{env.user\_id}, and \texttt{queryOrders.qi.accountId}.
\end{itemize}

By processing each cluster independently, \lighttechname\ transforms one long and intractable log into multiple smaller and coherent inputs for the LLM. 
This greatly reduces the risk of exceeding the model’s context window, while also making comparisons between related fields more explicit and interpretable. 

\paragraph{Attack Generation and Prompt Refinement.}
The second challenge concerns the limited reasoning ability of small models, which often fail to infer project-specific invariants. 
To address this, our approach introduces an iterative loop of \emph{attack generation} and \emph{prompt refinement}, as illustrated in Figure~\ref{fig:attack_refinement}. 

The core idea is to leverage large models to automatically generate targeted attack logs from normal logs. 
These generated attacks often exploit missing invariants in the current prompt configuration. 
By checking the invariants produced by the small model against the generated attack logs, we can identify undetected attacks. 
These undetected cases are then fed back into a large model to refine the prompt, which is subsequently reused by the small model to regenerate invariants. 
This iterative process allows us to gradually learn project-specific invariant patterns and to enrich the detection coverage. 

For instance, in the motivating example, the attack generation step can synthesize an adversarial log where the \texttt{cancelOrder.arguments.orderId} does not appear in the order list returned by \texttt{queryOrders}. 
If the current prompt fails to enforce this invariant, the refinement step will propose a new rule such as:
\begin{quote}
\emph{``Ensure that every \texttt{cancelOrder.arguments.orderId} matches one of the values in \texttt{queryOrders.response.data.id}.''}
\end{quote}
Once integrated, this refined prompt enables the small model to generate the correct invariant and to successfully detect the motivating attack. 

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.85\linewidth]{figures/attack_refinement.pdf}
    \caption{Attack generation and prompt refinement loop. Large models generate targeted attacks; undetected cases are used to refine prompts, improving small-model invariant generation.}
    \label{fig:attack_refinement}
\end{figure}
