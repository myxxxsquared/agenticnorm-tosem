\section{Motivating Example}

To illustrate the challenges of inferring invariants from API logs, we consider a representative example from a railway ticketing system. 

\paragraph{Example Scenario.} 
Suppose a user issues two consecutive API calls: \texttt{queryOrders} to retrieve their order history, followed by \texttt{cancelOrder} to cancel a specific ticket. The corresponding log entries are shown below.


From these two logs, one natural invariant is that the \texttt{cancelOrder.arguments.orderId} must match one of the order identifiers returned by \texttt{queryOrders.response.data.id}. This invariant captures the semantic relationship between the two APIs.

\paragraph{Challenges for Small Models.} 
Automatically discovering such invariants is far from trivial. Small models face two main limitations:

\begin{itemize}
    \item \textbf{Context Length Limit.} 
    Even this small motivating example already spans hundreds of tokens due to nested JSON structures. In practice, real-world logs are often far longer, since a single \texttt{queryOrders} response may return dozens of orders, each with multiple fields. This quickly pushes the log length to several thousand tokens. We empirically observed that a significant portion of logs directly exceed the modelâ€™s maximum context length, making it impossible to process them in full.

    Table~\ref{tab:log_length} summarizes the proportion of logs that exceed the context window. The second column shows the percentage of logs longer than the global maximum context length, while the third column shows the proportion of logs that cannot even fit into a high-capacity local model (Gemma3-27B).
    
    \begin{table}[h]
    \centering
    \caption{Proportion of logs exceeding length constraints.}
    \label{tab:log_length}
    \begin{tabular}{lcc}
    \toprule
    \textbf{Dataset} & \textbf{\% logs $>$ global max length} & \textbf{\% logs $>$ Gemma3-27B capacity} \\
    \midrule
    TrainTicket & --\% & --\% \\
    NiceFish    & --\% & --\% \\
    Other       & --\% & --\% \\
    \bottomrule
    \end{tabular}
    \end{table}

    \item \textbf{Reasoning Ability.} 
    Small models heavily rely on manually crafted prompts. They often fail to recognize cross-API relationships, such as that the cancellation target must belong to the queried order set. In many cases, the model simply ``does not know what to infer,'' leading to missing or incorrect invariants.
\end{itemize}

\paragraph{Hardware Capacity Analysis.}
When deploying locally, GPU memory further constrains the usable context length. Consider a single NVIDIA RTX 6000 Ada GPU with \textbf{49140MiB} memory. Running a 27B-parameter model such as \textbf{Gemma3-27B}, the maximum supported context window is roughly
\[
\text{MaxTokens} \approx \frac{49{,}140 \text{ MB}}{\text{Memory per token}}.
\]
Following standard scaling estimates, Gemma3-27B on 49GB VRAM can typically accommodate around \textbf{16k--32k tokens} in full precision, depending on batch size and precision optimizations. However, our empirical distribution shows that a non-negligible fraction of logs already exceed this limit. The last column of Table~\ref{tab:log_length} quantifies this effect. 

\paragraph{Our Solution.}
To address these challenges, we propose two complementary strategies:
\begin{enumerate}
    \item \textbf{Field Clustering.} To mitigate context length issues, we cluster comparable fields together. Instead of providing the full log, we expand the structures and extract only relevant fields for invariant inference.
    \item \textbf{Refined Prompts via Generated Attacks.} To strengthen reasoning, we refine prompts by leveraging automatically generated attacks. These adversarial cases expose missing invariants, which in turn help the model generalize and infer more comprehensive rules.
\end{enumerate}

This motivating example highlights both the importance of semantic invariants in API interactions and the inherent difficulties faced by small models, motivating the need for our proposed approach.