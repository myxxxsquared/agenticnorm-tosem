\section{Motivating Example}

\input{floats/02-01-motivation-example}

To illustrate the challenges of detecting anomalies from API logs, we examine a case from the \trainticket dataset~\cite{trainticketsystem} involving a compromised ticket refund process. 
We focus on two backend APIs recorded in the logs: 
\texttt{/api/v1/queryOrders} (\texttt{queryOrders}) and \texttt{/api/v1/cancelOrder} (\texttt{cancelOrder}). 
Figure~\ref{fig:motivating_example} shows a simplified version of the relevant log entries, while the full logs are available in our anonymous artifact~\cite{agenticnorm-website}.  

In a normal workflow, cancellation requires two steps. 
First, the user invokes \texttt{queryOrders} to retrieve a list of refundable tickets, each identified by a unique \texttt{orderId}. 
Second, the user selects one of these tickets and submits a cancellation request via \texttt{cancelOrder}. 
In this case, the \texttt{orderId} used in the cancellation request (e.g., \texttt{fe9c72d9}) must match one of the identifiers returned by \texttt{queryOrders}.  

However, because frontend code executes entirely on the client side, a malicious user can tamper with browser data and forge unauthorized requests. 
For example, the attacker may replace the valid \texttt{orderId} with an arbitrary identifier not returned by \texttt{queryOrders} (e.g., \texttt{418ea03c}). 
This manipulation allows the attacker to cancel a ticket they do not own or to repeat a cancellation that should not be permitted. 
Such behavior can cause duplicate refunds and financial losses, creating significant risks to system security and integrity.  

\subsection{Background}
APIs serve as the communication interface between frontend and backend components, typically defined by a request path and a payload format. 
Logs record the actual data exchanged through these APIs, usually including the request path and a JSON object for both request and response. 
Thus, logs are commonly represented in structured JSON format.  

In our example, the first API, \texttt{queryOrders}, returns a list of refundable tickets, while the second API, \texttt{cancelOrder}, takes an \texttt{orderId} as input to process a cancellation. 
Under normal operation, the \texttt{orderId} in \texttt{cancelOrder} must be one of those returned by \texttt{queryOrders}. 
Logs generated under such normal interactions are considered benign, whereas logs generated by tampering or replaying requests on the client side are regarded as abnormal.  

The relation that \texttt{cancelOrder.arguments.orderId} must match one of \texttt{queryOrders.results[].id} is an example of a cross-API dependency. 
WebNorm~\cite{liao2024detecting} refers to such relations as \emph{dependencies} and identifies them via program analysis, source code instrumentation, or other static/dynamic methods. 
Once the dependency is known, any violation observed in the logs can be flagged as abnormal behavior.  

\subsection{Existing Approaches}  
Existing log-based anomaly detection methods fall into two categories:  
(1) \textbf{Model-learning-based detectors}, which learn embeddings or features from normal and abnormal logs to classify anomalies; and  
(2) \textbf{Invariant-learning-based detectors}, which derive semantic invariants from logs and flag violations.  

Model-learning-based detectors struggle in this scenario because normal and abnormal logs differ in only a few fields, making them nearly indistinguishable in embedding space. 
Furthermore, anomalies may be buried within long sequences of interleaved events, diluting the anomaly signal.  

WebNorm~\cite{liao2024detecting} addresses this limitation by learning semantic invariants from logs with the help of LLMs. 
For example, it can infer the constraint that \texttt{cancelOrder.arguments.orderId} must match one of \texttt{queryOrders.results[].id}. 
By mapping code-level data flows to log fields and validating them as invariants, WebNorm can flag any violations as anomalies. 
In our motivating case, WebNorm successfully learns the cross-API dependency and detects the abnormal cancellation attempt.  

Despite its effectiveness, WebNorm suffers from three key limitations:  
\begin{itemize}
    \item \textbf{Dependence on program analysis and source code}: it requires access to and instrumentation of frontend/backend code, which is costly, fragile under rapid iteration, and infeasible for closed-source or third-party systems.  
    \item \textbf{Reliance on heavyweight proprietary LLMs}: invariant synthesis depends on remote, closed-source models, leading to latency, cost, and compliance/privacy risks. Real logs are long and deeply nested, often exceeding the context windows of compact models.  
    \item \textbf{Prompt sensitivity}: correct invariants often appear only with carefully engineered prompts, making the process labor-intensive, project-specific, and difficult to generalize.  
\end{itemize}  

\subsection{Our Approach}  

We aim to preserve WebNorm’s strength in capturing \emph{consistency constraints} while addressing its limitations. 
Unlike WebNorm, our approach relies only on logs (without program analysis), operates primarily with compact, locally deployable LLMs, and mitigates prompt fragility by automatically refining prompts through generated abnormal logs.  

Specifically, we propose two techniques to enable anomaly detection with compact models:  

\begin{itemize}
    \item \textbf{Field Clustering.} Each JSON log entity is expanded into flattened fields and grouped into small, semantically coherent clusters. Instead of feeding the full log into the model, each cluster is processed independently. This reduces context length, highlights meaningful field-level relationships, and allows compact models to handle long and complex logs more effectively.  
    \item \textbf{Prompt Refinement via Generated Attacks.} To reduce reliance on manual prompt engineering, we design prompts that guide the LLM to generate abnormal logs. These generated logs expose missing constraints, which are then used to refine the prompts. The refined prompts enable compact models to capture project-specific invariants more accurately and robustly.  
\end{itemize}  

\paragraph{Field Clustering}  
To efficiently adapt logs for compact models, we expand each JSON record into individual fields and then group comparable ones into clusters. 
Figure~\ref{fig:motivating_example_clustering} illustrates this process. Each cluster is given as a separate input to the LLM, ensuring that related fields are explicitly compared while avoiding unnecessary context.  

\input{floats/02-02-motivation-example-clustering}  

\paragraph{Prompt Refinement via Generated Attacks}  
Compact models generally lack strong reasoning ability and cannot reliably infer constraints from fixed prompts. 
Manual prompt adjustment is time-consuming and project-specific.  
Figure~\ref{fig:motivating_example_prompt} (left) shows the original WebNorm prompt, which depends on handcrafted instructions and examples.  
Our approach (middle) augments the prompt with additional instructions that guide the model to generate abnormal logs. These abnormal logs force the model to reason about field relationships—for example, checking consistency between joined fields and their originals.  
As shown in Figure~\ref{fig:motivating_example_prompt} (right), the refined prompt enables the compact model to generate invariants that successfully capture the required field-level constraints.  

\input{floats/02-03-motivation-example-prompt}  

