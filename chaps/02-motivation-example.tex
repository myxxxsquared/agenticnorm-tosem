\section{Motivating Example}

To illustrate the challenges of inferring invariants from API logs, we consider a representative example from a railway ticketing system. 

\paragraph{Example Scenario.} 
Suppose a user issues two consecutive API calls: \texttt{queryOrders} to retrieve their order history, followed by \texttt{cancelOrder} to cancel a specific ticket. The corresponding log entries are shown below.


From these two logs, one natural invariant is that the \texttt{cancelOrder.arguments.orderId} must match one of the order identifiers returned by \texttt{queryOrders.response.data.id}. This invariant captures the semantic relationship between the two APIs.

\paragraph{Challenges for Small Models.} 
Automatically discovering such invariants is far from trivial. Small models face two main limitations:

\begin{itemize}
    \item \textbf{Context Length Limit.} 
    Even this small motivating example already spans hundreds of tokens due to nested JSON structures. In practice, real-world logs are often far longer, since a single \texttt{queryOrders} response may return dozens of orders, each with multiple fields. This quickly pushes the log length to several thousand tokens. We empirically observed that a significant portion of logs directly exceed the model’s maximum context length, making it impossible to process them in full.

    Table~\ref{tab:log_length} summarizes the proportion of logs that exceed the context window. The second column shows the percentage of logs longer than the global maximum context length, while the third column shows the proportion of logs that cannot even fit into a high-capacity local model (Gemma3-27B).
    
    \begin{table}[h]
    \centering
    \caption{Proportion of logs exceeding length constraints.}
    \label{tab:log_length}
    \begin{tabular}{lcc}
    \toprule
    \textbf{Dataset} & \textbf{\% logs $>$ global max length} & \textbf{\% logs $>$ Gemma3-27B capacity} \\
    \midrule
    TrainTicket & --\% & --\% \\
    NiceFish    & --\% & --\% \\
    Other       & --\% & --\% \\
    \bottomrule
    \end{tabular}
    \end{table}

    \item \textbf{Reasoning Ability.} 
    Small models heavily rely on manually crafted prompts. They often fail to recognize cross-API relationships, such as that the cancellation target must belong to the queried order set. In many cases, the model simply ``does not know what to infer,'' leading to missing or incorrect invariants.
\end{itemize}

\paragraph{Hardware Capacity Analysis.}
When deploying locally, GPU memory further constrains the usable context length. Consider a single NVIDIA RTX 6000 Ada GPU with \textbf{49140MiB} memory. Running a 27B-parameter model such as \textbf{Gemma3-27B}, the maximum supported context window is roughly
\[
\text{MaxTokens} \approx \frac{49{,}140 \text{ MB}}{\text{Memory per token}}.
\]
Following standard scaling estimates, Gemma3-27B on 49GB VRAM can typically accommodate around \textbf{16k--32k tokens} in full precision, depending on batch size and precision optimizations. However, our empirical distribution shows that a non-negligible fraction of logs already exceed this limit. The last column of Table~\ref{tab:log_length} quantifies this effect. 

\paragraph{Our Solution.}
To address these challenges, we propose two complementary strategies:
\begin{enumerate}
    \item \textbf{Field Clustering.} To mitigate context length issues, we cluster comparable fields together. Instead of providing the full log, we expand the structures and extract only relevant fields for invariant inference.
    \item \textbf{Refined Prompts via Generated Attacks.} To strengthen reasoning, we refine prompts by leveraging automatically generated attacks. These adversarial cases expose missing invariants, which in turn help the model generalize and infer more comprehensive rules.
\end{enumerate}

This motivating example highlights both the importance of semantic invariants in API interactions and the inherent difficulties faced by small models, motivating the need for our proposed approach.

\paragraph{Field Clustering with \lighttechname.}
To overcome the context length limitation, our approach, \lighttechname, introduces a \emph{field clustering} mechanism. 
Instead of feeding the entire long log into the model, we decompose the original log into a set of smaller clusters. 
Each cluster groups together only the fields that are semantically comparable across APIs. 

For example, in the motivating case of \texttt{cancelOrder} and \texttt{queryOrders}, the raw log is long and contains numerous nested fields. 
\lighttechname\ automatically partitions these fields into multiple clusters:
\begin{itemize}
    \item \textbf{Cluster 1:} focuses on order identifiers, linking \texttt{cancelOrder.arguments.orderId} with \texttt{queryOrders.response.data.id}.
    \item \textbf{Cluster 2:} focuses on status values, linking \texttt{cancelOrder.response.status} with \texttt{queryOrders.response.data.status}.
    \item \textbf{Cluster 3:} focuses on user and account identity, linking \texttt{cancelOrder.arguments.loginId}, \texttt{env.user\_id}, and \texttt{queryOrders.qi.accountId}.
\end{itemize}

By processing each cluster independently, \lighttechname\ transforms one long and intractable log into multiple smaller and coherent inputs for the LLM. 
This greatly reduces the risk of exceeding the model’s context window, while also making comparisons between related fields more explicit and interpretable. 

\paragraph{Attack Generation and Prompt Refinement.}
The second challenge concerns the limited reasoning ability of small models, which often fail to infer project-specific invariants. 
To address this, our approach introduces an iterative loop of \emph{attack generation} and \emph{prompt refinement}, as illustrated in Figure~\ref{fig:attack_refinement}. 

The core idea is to leverage large models to automatically generate targeted attack logs from normal logs. 
These generated attacks often exploit missing invariants in the current prompt configuration. 
By checking the invariants produced by the small model against the generated attack logs, we can identify undetected attacks. 
These undetected cases are then fed back into a large model to refine the prompt, which is subsequently reused by the small model to regenerate invariants. 
This iterative process allows us to gradually learn project-specific invariant patterns and to enrich the detection coverage. 

For instance, in the motivating example, the attack generation step can synthesize an adversarial log where the \texttt{cancelOrder.arguments.orderId} does not appear in the order list returned by \texttt{queryOrders}. 
If the current prompt fails to enforce this invariant, the refinement step will propose a new rule such as:
\begin{quote}
\emph{``Ensure that every \texttt{cancelOrder.arguments.orderId} matches one of the values in \texttt{queryOrders.response.data.id}.''}
\end{quote}
Once integrated, this refined prompt enables the small model to generate the correct invariant and to successfully detect the motivating attack. 

\begin{figure}[t]
    \centering
    % \includegraphics[width=0.85\linewidth]{figures/attack_refinement.pdf}
    \caption{Attack generation and prompt refinement loop. Large models generate targeted attacks; undetected cases are used to refine prompts, improving small-model invariant generation.}
    \label{fig:attack_refinement}
\end{figure}
