\section{Related Work}

\paragraph{Log Anomaly Detection.}
Early attempts at anomaly detection from logs were largely based on analyzing execution traces of systems or individual APIs, with the goal of spotting deviations from expected behaviors~\cite{ye2024spurious}. 
Classical approaches mostly relied on manually written rules or statistical thresholds~\cite{hansen1993automated,oprea2015detection,prewett2003analyzing,rouillard2004real,roy2015perfaugur,yamanishi2005dynamic,yen2013beehive}. 
While useful in certain domains, these methods require expert-crafted specifications and often fail to generalize across applications.

With the advent of data-driven methods, researchers shifted towards models that automatically learn behavioral patterns from logs~\cite{acharya2007mining,lorenzoli2008automatic,walkinshaw2008inferring,pradel2009automatic,beschastnikh2011leveraging,krka2014automatic,breier2015anomaly,amar2018using,rufino2020improving,stocco2020towards,kang2019spatiotemporal,njoku2025kernel,wu2023effectiveness,lupton2021literature,alam2019framework,schneider2010synoptic}. 
A large body of work in this direction adopts deep learning models to classify log sequences as normal or anomalous. Recurrent architectures have been widely explored to capture sequential dependencies~\cite{du2017deeplog,brown2018recurrent}, while CNN-based solutions exploit local contextual signals~\cite{lu2018detecting,fu2023mlog}. 
More recent efforts employ Transformers~\cite{huang2020hitanomaly,guo2024logformer}, graph neural networks~\cite{zhang2022deeptralog}, or pretrained language models tailored for log data~\cite{guo2021logbert,han2023loggpt}. 
Some studies further reduce labeling overhead by adopting unsupervised or semi-supervised formulations~\cite{yang2021plelog,meng2019loganomaly}. 
Despite their predictive strength, these neural methods often act as black boxes, providing little insight into the root cause of anomalies and occasionally overlooking subtle yet critical deviations.

A smaller set of research emphasizes explainability by constructing explicit normality constraints. The most prominent example is WebNorm~\cite{liao2024detecting}, which encodes behavioral invariants of web systems as first-order logic rules derived from logs. 
This line of work demonstrates that logic-based reasoning can both detect anomalies and provide interpretable explanations. 
However, WebNorm relies heavily on source code analysis and large external models, and it does not explicitly enforce the consistency between logs and their underlying data sources. 
Our work builds on this direction, proposing techniques that refine normality inference with lightweight, deployable models and enhanced relational reasoning.

\paragraph{RESTful API Security.}
RESTful APIs, by virtue of their statelessness and ubiquity, have become an essential surface for attacks in modern web systems. 
Prior research has extensively explored automated vulnerability discovery through API testing and fuzzing~\cite{deng2023nautilus,du2024vulnerability,atlidakis2019restler,viglianisi2020resttestgen,martin2020restest,martin2020automated}. 
These methods typically mutate request sequences or payloads to trigger failures, guided by API specifications, dependency constraints~\cite{viglianisi2020resttestgen,martin2020restest}, or machine learning predictions~\cite{lyu2023miner}. 
Enhanced strategies further refine the fuzzing process to target specific classes of vulnerabilities such as injection attacks or cross-site scripting~\cite{deng2023nautilus,du2024vulnerability}.

While fuzzing uncovers flaws through active probing, our approach takes a complementary perspective: we aim to strengthen API security by learning invariants that characterize normal interaction patterns. 
By detecting deviations from these learned normalities, we provide a systematic way to capture tampering behaviors that bypass conventional fuzzing-based detection.

\paragraph{Comparison to Our Work.}
Existing solutions for log anomaly detection and RESTful API security either emphasize predictive accuracy through deep learning or rely on fuzzing techniques to expose vulnerabilities. 
While effective to some extent, these approaches face notable limitations: deep neural methods lack interpretability and often miss subtle invariants, whereas fuzzing uncovers vulnerabilities opportunistically but does not generalize to unseen tampering strategies. 
WebNorm represents an important step toward explainable anomaly detection, but its reliance on heavyweight, closed-source models and program source code restricts its applicability in practice. 

In contrast, our approach focuses on lightweight, locally deployable models integrated into a multi-agent framework. 
By introducing \emph{field clustering}, we address the long-context challenge inherent in compact models, and by leveraging an iterative \emph{attack-driven prompt refinement loop}, we enable the system to self-improve without extensive manual intervention. 
This combination not only preserves explainability but also ensures that detection can be deployed securely and efficiently in real-world settings.
