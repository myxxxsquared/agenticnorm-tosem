
% {\color{red}{TODOS: 1) replace all "attack" to abnormal. 2) replace invaraints to constraints 3) place novelty to adversal learning. 4) add more examples. 5) add more details in method}}

\section{Introduction}

Web applications play a critical role in modern infrastructures, supporting domains such as finance~\cite{feyen2021fintech,vukovic2025ai}, e-commerce~\cite{rahman2022revolutionizing}, and healthcare~\cite{lazakidou2009web}. Their reliability and security are of paramount importance. Unfortunately, web frontends are inherently manipulable: attackers can alter client-side code or parameters to bypass validations, tamper with workflows, or inject abnormal behaviors.  

In principle, backend applications implement authorization checks and other safeguards to prevent such manipulations. However, because abnormal behaviors are often difficult to exhaustively cover through conventional frontend testing, certain anomalous actions may escape detection by the backend, leaving exploitable vulnerabilities.  

To mitigate such risks, log analysis has become an effective defense mechanism. Logs in web systems typically record detailed interactions between clients and servers, including API calls, request parameters, and response statuses. Analyzing these logs enables the identification of anomalous interaction patterns that may signal security threats~\cite{yen2013beehive,alam2019framework}. Current state-of-the-art log analysis methods can be broadly classified into two categories: (1) \emph{model-learning-based approaches}, which train predictive models from normal logs and use them to detect anomalies~\cite{acharya2007mining,lorenzoli2008automatic,walkinshaw2008inferring,pradel2009automatic,beschastnikh2011leveraging,krka2014automatic,breier2015anomaly,amar2018using,rufino2020improving,stocco2020towards,kang2019spatiotemporal,njoku2025kernel,wu2023effectiveness,lupton2021literature,alam2019framework,schneider2010synoptic}, and (2) \emph{rule-learning-based approaches}, which mine logical constraints from logs and detect violations~\cite{liao2024detecting}.  

The first category, model-learning-based approaches, typically employ deep learning models to classify logs as normal or abnormal~\cite{du2017deeplog,brown2018recurrent}. While effective in many scenarios, these approaches suffer from limited interpretability: the output is often a binary decision without clear explanations of the underlying cause. Given the large volume of logs in practice, even a small false positive rate can result in overwhelming numbers of alerts, complicating deployment. Moreover, these methods often struggle to detect subtle anomalies~\cite{no2024training}; when malicious modifications closely mimic normal behaviors, the models tend to misclassify them, leading to missed detections.  

To address these limitations, rule-learning-based approaches attempt to extract explicit logical constraints from logs and use them for anomaly detection~\cite{liao2024detecting}. Such approaches provide better interpretability, as the violated constraint reveals the concrete reason for detection. For example, WebNorm detects anomalies like the \trainticket case by learning cross-API constraints (e.g., \texttt{cancelOrder.arguments.orderId} must appear in \texttt{queryOrders.results[].id}). Despite its effectiveness, however, WebNorm suffers from three critical limitations:  

\begin{itemize}
    \item \textbf{Dependence on program analysis and source code}: it requires access to frontend/backend code and additional instrumentation to align logs with code-level workflows, which is costly, brittle under rapid iteration, and infeasible for closed-source or third-party components.  
    \item \textbf{Reliance on heavyweight proprietary LLMs}: constraint confirmation and synthesis depend on large closed-source models, which introduce latency, cost, and compliance/privacy risks. Furthermore, real logs are long and deeply nested, often exceeding the context windows of compact models and forcing reliance on heavyweight remote services.  
    \item \textbf{Prompt sensitivity}: generating correct constraints often requires carefully crafted prompts. This results in project-specific manual engineering with poor transferability and unstable outcomes.  
\end{itemize}  

In this paper, we propose \lighttechname, a lightweight anomaly detection framework designed around compact, locally deployable LLMs. Unlike WebNorm, \lighttechname avoids dependence on program analysis, removes the need for heavyweight proprietary models, and mitigates prompt fragility through three key techniques:  

\begin{itemize}
    \item \textbf{Eliminating source-code dependence.} Instead of relying on program instrumentation to build log–code mappings, \lighttechname directly infers constraints from raw logs. It discovers inter-API relationships using frequency-based analysis of co-occurring calls and derives constraints purely from runtime behaviors, enabling applicability even when source code is unavailable.  
    \item \textbf{Field clustering for context reduction.} To overcome compact models’ limitations on long and nested logs, \lighttechname expands JSON entities into flattened fields and groups them into semantically coherent clusters. Each cluster is processed independently, greatly reducing context length while preserving meaningful comparisons. This allows lightweight models to handle large-scale logs without resorting to heavyweight external LLMs.  
    \item \textbf{Prompt refinement via generated abnormals.} To address prompt fragility, \lighttechname introduces an iterative loop where adversarial logs are automatically generated to expose missing constraints. These logs guide the refinement of prompts, producing project-specific instructions that are both stable and transferable across iterations. This enables compact models to progressively capture robust invariants without manual prompt engineering.  
\end{itemize}  

\lighttechname integrates these components into a multi-agent workflow, where agents for invariant generation, attack generation, and prompt refinement cooperate iteratively. The result is a system that adapts and self-improves with minimal human intervention.  

\paragraph{Contributions.} This paper makes the following contributions:  
\begin{itemize}
    \item We propose \lighttechname, a multi-agent framework for web anomaly detection that eliminates dependency on application source code.  
    \item We implement the \lighttechname framework and evaluate it on real-world benchmarks, including TrainTicket and NiceFish.  
    \item Experimental results show that \lighttechname achieves more effective anomaly detection with reduced contextual requirements compared to existing approaches.  
\end{itemize}  
