
% {\color{red}{TODOS: 1) replace all "attack" to abnormal. 2) replace invaraints to constraints 3) place novelty to adversal learning. 4) add more examples. 5) add more details in method}}

\section{Introduction}

Web applications play a critical role in modern infrastructures, supporting domains such as finance~\cite{feyen2021fintech,vukovic2025ai}, e-commerce~\cite{rahman2022revolutionizing}, and healthcare~\cite{lazakidou2009web}. Their reliability and security are of paramount importance. Unfortunately, web frontends are inherently manipulable: attackers can alter client-side code or parameters to bypass validations, tamper with workflows, or inject attack behaviors.  

In principle, backend applications implement authorization checks and other safeguards to prevent such manipulations. However, because attack behaviors are often difficult to exhaustively covered through conventional frontend testing, certain attack actions may escape detection by the backend, leaving exploitable vulnerabilities.  
To mitigate such risks, log analysis has become an effective defense mechanism. Logs in web systems typically record detailed interactions between clients and servers, including API calls, request parameters, and response statuses. Analyzing these logs enables the identification of attack interaction patterns that may signal security threats~\cite{yen2013beehive,alam2019framework}. Existing log analysis approaches typically learn models from logs to detect anomalies. These \emph{model-learning-based approaches} fall into two main paradigms: (1) training neural network models (such as deep learning classifiers) from normal logs to predict anomalies~\cite{acharya2007mining,lorenzoli2008automatic,walkinshaw2008inferring,pradel2009automatic,beschastnikh2011leveraging,krka2014automatic,breier2015anomaly,amar2018using,rufino2020improving,stocco2020towards,kang2019spatiotemporal,njoku2025kernel,wu2023effectiveness,lupton2021literature,alam2019framework,schneider2010synoptic,du2017deeplog,brown2018recurrent}, and (2) directly using large language models (LLMs) to judge whether logs contain anomalies.

While effective in many scenarios, these model-learning-based approaches suffer from significant limitations. First, they have limited interpretability: the output is often a binary decision without clear explanations of the underlying cause. Given the large volume of logs in practice, even a small false positive rate can result in overwhelming numbers of alerts, complicating deployment. Moreover, these methods often struggle to detect subtle anomalies~\cite{no2023rapid}; when malicious modifications closely mimic normal behaviors, the models tend to misclassify them, leading to missed detections.  

To address these limitations, our prior work WebNorm~\cite{liao2024detecting} proposed a \emph{rule-learning-based approach} that extracts explicit logical constraints from logs for anomaly detection. Unlike model-learning approaches, WebNorm combines program analysis and LLM inference: it first analyzes source code to identify how data flow across APIs, and then employs manually crafted prompts to guide the LLM in generating explicit constraints. For example, WebNorm can infer constraints like API1's input field A should match API2's output field B. This provides strong interpretability, as a violated constraint points to the concrete reason for detection. However, despite these advantages, WebNorm faces three key limitations that hinder its broader applicability and practical deployment:

\begin{itemize}
    \item \textbf{Dependence on program analysis and source code}: it requires access to frontend/backend code and derives event relationships using program analysis by analyzing how data flow across API calls and execution traces, which is costly for deployment in new systems, and infeasible for closed-source or third-party components.
    \item \textbf{Reliance on heavyweight proprietary LLMs}: constraint confirmation and synthesis depend on heavyweight proprietary models, which introduce latency, cost, and compliance/privacy risks. Furthermore, real logs are long and deeply nested, often exceeding the context windows of lightweight models and forcing reliance on heavyweight remote services.  
    \item \textbf{Prompt sensitivity}: generating correct constraints often requires carefully crafted prompts. This results in project-specific manual engineering with poor transferability and unstable outcomes.  
\end{itemize}  

In this paper, we present \lighttechname, a significant extension of WebNorm that addresses these limitations through a lightweight anomaly detection framework. Building upon WebNorm's foundation of constraint-based anomaly detection, \lighttechname retains the interpretability of rule-based methods while removing the barriers to deployment. We propose \lighttechname with three integrated components that directly address the above challenges: (1) \textbf{frequency-based inter-API relation discovery}, which operates entirely on logs to uncover inter-API relationships through co-occurrence analysis, eliminating the need for source code access; (2) \textbf{field clustering}, which expands nested log entities into flattened fields and groups them into semantically coherent clusters, enabling lightweight models to handle large-scale logs by reducing input length while preserving meaningful comparisons; and (3) \textbf{prompt refinement via generated attacks}, which employs an iterative loop where adversarially synthesized logs reveal missing constraints and enrich prompts with more explicit descriptions, progressively improving constraint quality without manual engineering.

\lighttechname integrates these components into a novel multi-agent workflow, where agents for constraint generation, attack generation, and prompt refinement cooperate iteratively. The system thus adapts and self-improves with minimal human intervention, enabling effective and interpretable anomaly detection using only lightweight, locally deployable LLMs, eliminating WebNorm's dependence on heavyweight proprietary models.

This paper, as a significant extension of WebNorm, makes the following contributions:
\begin{itemize}
    \item We present \lighttechname, a lightweight extension of WebNorm that integrates three key components: (1) frequency-based inter-API relation discovery for eliminating source code dependence, (2) field clustering for enabling lightweight models to handle large-scale logs, and (3) prompt refinement via generated attacks for overcoming manual prompt engineering. Together, these components make \lighttechname broadly applicable to closed-source and third-party systems while using only lightweight, locally deployable LLMs.
    \item We implement the complete \lighttechname framework as a novel multi-agent workflow where agents for constraint generation, attack generation, and prompt refinement cooperate iteratively. The source code and datasets are publicly available in our repository~\cite{agenticnorm-website}.  
    \item We conduct comprehensive and extensive experiments on two widely used benchmarks, TrainTicket and NiceFish, covering over 220k log entries and 230 attack cases with detailed ablation studies and comparative analyses. Results demonstrate that \lighttechname significantly outperforms the original WebNorm, achieving F1-scores of 0.92 and 0.85 compared to WebNorm's 0.88 and 0.75, while requiring no source code access and using only lightweight models.  
\end{itemize}
