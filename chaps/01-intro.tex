
{\color{red}{TODOS: 1) replace all "attack" to abnormal. 2) replace invaraints to constraints}}

\section{Introduction}

Web applications play a critical role in modern infrastructures, supporting domains such as finance, e-commerce, and healthcare. Their reliability and security are of paramount importance. Unfortunately, web frontends are inherently manipulable: attackers can alter client-side code or parameters to bypass validations, tamper with workflows, or inject abnormal behaviors. These manipulations often manifest as subtle anomalies in backend logs, making detection both crucial and challenging. Traditional log-based anomaly detection methods, such as rule-based systems (e.g., Splunk, QRadar) and deep learning models (e.g., DeepLog, LogAnomaly, LogRobust), face three major obstacles: (1) subtle abnormalities may be indistinguishable from normal variations, (2) distribution shifts caused by evolving tamper strategies degrade model reliability, and (3) most solutions provide limited explainability, hindering root cause.

To address these limitations, recent work introduced WebNorm, an LLM-based anomaly detection framework that learns semantic invariants from web logs. WebNorm detects attacks such as the TrainTicket example by learning cross-API constraints (e.g., ensuring that \texttt{cancelOrder.arguments.orderId} must appear in \texttt{queryOrders.results[].id}). While effective, WebNorm requires program analysis and source-code instrumentation to align logs with code-level flows, and relies on large, closed-source LLMs for invariant synthesis. These dependencies introduce deployment barriers, raise privacy concerns, and limit scalability. Moreover, its detection quality is highly sensitive to manual prompt engineering, which is brittle and hard to transfer across projects.

In this paper, we propose \lighttechname, a lightweight anomaly detection framework designed around compact, locally deployable LLMs. \lighttechname eliminates source-code dependence and reduces reliance on manual prompt tuning through two key techniques:

\begin{itemize}
    \item \textbf{Field Clustering for Context Reduction.} Compact models struggle with long, complex logs. We expand each JSON log entity into flattened fields and group related ones into semantically coherent clusters. Each cluster is processed independently, which shortens the context window and makes field-level comparisons more explicit.
    \item \textbf{Prompt Refinement via Generated Attacks.} Compact models have limited reasoning ability and cannot reliably infer field constraints from fixed prompts. We address this by automatically generating adversarial logs to reveal missing invariants. These logs guide an iterative refinement of prompts, enabling the compact model to capture project-specific constraints more effectively.
\end{itemize}

\lighttechname integrates these components into a multi-agent workflow, where agents for invariant generation, attack generation, and prompt refinement cooperate iteratively. The result is a system that adapts and self-improves without extensive human involvement.

\paragraph{Contributions.} This paper makes the following contributions:
\begin{itemize}
    \item We propose \lighttechname, a multi-agent framework for web anomaly detection that eliminates the dependency on application source code.
    \item We implement the \lighttechname framework and evaluate it on real-world benchmarks, including TrainTicket and NiceFish.  
    \item Experimental results demonstrate that \lighttechname requires less contextual information while achieving more effective anomaly detection compared to existing approaches.
\end{itemize}
