\section{Discussion}

\textbf{Why can adversarial attacks improve anomaly detection efficiency?}

Adversarial attacks play a crucial role in refining the prompts used for invariant generation. At the initial stage (Round 0), the prompt may fail to capture critical invariants, leading to missed detections for certain types of tamper attacks. However, when we introduce adversarial attacks that exploit these weaknesses, the system is forced to adapt: the failure cases serve as concrete counterexamples that guide the prompt-refinement process. After just one refinement iteration (Round 1), the updated prompt can successfully detect the previously missed anomaly. 

For example, in Round 0, \lighttechname may fail to detect an attack where [\texttt{PLACEHOLDER\_EXAMPLE}], but after one iteration of adversarial attackâ€“guided refinement, the system adjusts the prompt and captures the invariant necessary to identify this anomaly. This self-improving loop demonstrates how adversarial attacks not only test the robustness of the system but also actively drive the enhancement of detection accuracy, ultimately reducing the need for manual intervention and improving efficiency in practice.

\textbf{Rounds of refinement.}

\input{floats/05-01-rounds}

\input{floats/05-02-tokens}

Figure~\ref{fig:rounds-of-refinement} illustrates the impact of iterative prompt refinement on detection performance. We observe that performance improves steadily in the first three rounds, reaching a peak F1-score at Round 3. Beyond this point, performance begins to decline slightly. This trend can be attributed to the increasing length of prompts in later rounds (Figure~\ref{fig:number-of-tokens}), which may lead to overfitting or introduce unnecessary complexity that hampers the model's ability to generalize.